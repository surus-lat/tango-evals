2025-07-02:02:56:22,248 INFO     [__main__.py:279] Verbosity set to INFO
2025-07-02:02:56:26,412 INFO     [__init__.py:459] The tag 'teleia' is already registered as a group, this tag will not be registered. This may affect tasks you want to call.
2025-07-02:02:56:29,552 INFO     [__main__.py:376] Selected Tasks: ['aquas']
2025-07-02:02:56:29,559 INFO     [evaluator.py:164] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-07-02:02:56:29,559 INFO     [evaluator.py:201] Initializing hf model, with arguments: {'pretrained': 'sandbox-ai/Llama-3.1-Tango-70b-bnb_4b', 'parallelize': True, 'load_in_4bit': True}
2025-07-02:02:56:32,305 INFO     [huggingface.py:352] Model parallel was set to True, setting max memory per GPU to {0: 23784587264, 1: 24589893632, 2: 25029574656, 3: 25029574656} and device map to auto
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
/home/francis/lm-evaluation-harness/.venv/lib/python3.12/site-packages/transformers/quantizers/auto.py:222: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.
  warnings.warn(warning_msg)
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:03<00:24,  3.54s/it]Loading checkpoint shards:  25%|██▌       | 2/8 [00:07<00:21,  3.55s/it]Loading checkpoint shards:  38%|███▊      | 3/8 [00:10<00:17,  3.55s/it]Loading checkpoint shards:  50%|█████     | 4/8 [00:14<00:14,  3.53s/it]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:17<00:10,  3.53s/it]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:21<00:07,  3.53s/it]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:24<00:03,  3.53s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:28<00:00,  3.45s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:28<00:00,  3.50s/it]
2025-07-02:02:57:07,516 WARNING  [task.py:991] [Task: aquas] num_fewshot > 0 but fewshot_split is None. using preconfigured rule.
2025-07-02:02:57:07,516 WARNING  [task.py:325] [Task: aquas] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2025-07-02:02:57:07,516 WARNING  [task.py:991] [Task: aquas] num_fewshot > 0 but fewshot_split is None. using preconfigured rule.
2025-07-02:02:57:07,516 WARNING  [task.py:325] [Task: aquas] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.
2025-07-02:02:57:07,531 WARNING  [evaluator.py:270] Overwriting default num_fewshot of aquas from 1 to 1
2025-07-02:02:57:07,532 INFO     [task.py:415] Building contexts for aquas on rank 0...
  0%|          | 0/107 [00:00<?, ?it/s] 63%|██████▎   | 67/107 [00:00<00:00, 666.31it/s]100%|██████████| 107/107 [00:00<00:00, 713.46it/s]
2025-07-02:02:57:07,693 INFO     [evaluator.py:496] Running generate_until requests
Running generate_until requests:   0%|          | 0/107 [00:00<?, ?it/s]Running generate_until requests:   1%|          | 1/107 [00:23<41:15, 23.35s/it]Running generate_until requests:   2%|▏         | 2/107 [01:02<57:30, 32.86s/it]Running generate_until requests:   3%|▎         | 3/107 [01:27<50:19, 29.03s/it]Running generate_until requests:   4%|▎         | 4/107 [01:39<38:28, 22.41s/it]Running generate_until requests:   5%|▍         | 5/107 [01:57<35:29, 20.88s/it]Running generate_until requests:   6%|▌         | 6/107 [02:10<30:28, 18.10s/it]Running generate_until requests:   7%|▋         | 7/107 [02:28<30:14, 18.14s/it]Running generate_until requests:   7%|▋         | 8/107 [02:44<28:33, 17.31s/it]Running generate_until requests:   8%|▊         | 9/107 [03:18<37:09, 22.75s/it]Running generate_until requests:   9%|▉         | 10/107 [03:47<39:40, 24.54s/it]Running generate_until requests:  10%|█         | 11/107 [03:58<32:36, 20.38s/it]Running generate_until requests:  11%|█         | 12/107 [04:23<34:28, 21.77s/it]Running generate_until requests:  12%|█▏        | 13/107 [04:42<32:59, 21.06s/it]Running generate_until requests:  13%|█▎        | 14/107 [05:00<31:07, 20.08s/it]Running generate_until requests:  14%|█▍        | 15/107 [05:20<30:29, 19.89s/it]Running generate_until requests:  15%|█▍        | 16/107 [05:36<28:38, 18.89s/it]Running generate_until requests:  16%|█▌        | 17/107 [05:46<24:27, 16.30s/it]Running generate_until requests:  17%|█▋        | 18/107 [06:03<24:07, 16.27s/it]Running generate_until requests:  18%|█▊        | 19/107 [06:28<27:49, 18.97s/it]Running generate_until requests:  19%|█▊        | 20/107 [06:42<25:24, 17.53s/it]Running generate_until requests:  20%|█▉        | 21/107 [06:58<24:25, 17.05s/it]Running generate_until requests:  21%|██        | 22/107 [07:10<21:55, 15.48s/it]